model:
  hidden_size: 2880  # GPT-OSS-20B actual hidden size
  vocab_size: 201088 # GPT-OSS-20B actual vocab size
  notes_dim: 2048
  num_heads: 32
  plan_vocab_size: 65536
  plan_hash_salt: "parallel-decoder-v1"
  trunk:
    base_model: "gpt-oss-20b/original"
    torch_dtype: "bfloat16"
    device_map: null
    attn_implementation: "flash_attention_2"  # Use PyTorch Flex Attention (requires PyTorch 2.5+)
    gradient_checkpointing: true  # Trade compute for memory (Stage 4 trunk unfreezing)
    freeze_lower_layers: 4
    unfreeze_modules:
      - "lm_head"
      - "adapter"
  instrumentation:
    enabled: true
    top_k_layers: 8  # Instrument the top 8 layers
    gate_init: -4.0
  stream_adapters:
    hidden_size: 2880
    bottleneck_size: 512
    streams:
      - "stream_0"
      - "stream_1"
      - "stream_2"
    activation: "gelu"
    dropout: 0.05
  notes_bus:
    snapshot_dim: 2048
    max_snapshots: 4
    lag: 1
    dtype: "bfloat16"
  cross_attention:
    hidden_size: 2880
    notes_dim: 2048
    num_heads: 32
    gating_init: -4.0
  planner_head:
    hidden_size: 2880
    vocab_size: 201088  # Must match tokenizer vocab_size, not plan_vocab_size
    dropout: 0.0
  notes_head:
    hidden_size: 2880
    notes_dim: 2048
    dropout: 0.0
    gated: true
  speculation_head:
    hidden_size: 2880
    notes_dim: 2048
    dropout: 0.0
    teacher_scale: 1.0
  agreement_head:
    hidden_size: 2880
    dropout: 0.1
training:
  dataset_path: "data/processed/pdt_10k_gpt41/kd_train.jsonl"
  eval_dataset_path: "data/processed/pdt_10k_gpt41/kd_validation.jsonl"
  telemetry_dir: "experiments/gpt_oss"
  resume_from_checkpoint: true
  batch_size: 1  # Reduced from 2 for Stage 4 memory requirements
  grad_accumulation: 16  # Increased from 8 to maintain effective batch size
  learning_rate: 2.0e-4
  weight_decay: 0.01
  max_steps: 50000
  warmup_steps: 1250
  save_every: 2500
  log_interval: 25
  eval_interval: 10000
  device: null
  topology: all_to_all
  teacher:
    enabled: true
    type: "stop_grad"
    ema_decay: 0.995
  dataset_teacher:
    # Teacher notes are pre-generated during dataset pipeline (Stage 3)
    # No LLM calls happen during training
    cache_dir: "data/teacher_cache"
    max_snapshots: 4
    id_field: "example_id"
    refresh_cache: false
  curriculum:
    B: 4
    L: 32
    delta: 1
    rho_by_stream: {}
    steps_per_stage: 0
    stage_schedule:
      - 0     # Stage 0: Planner Pretrain (0-3749) ~6 epochs
      - 3750  # Stage 1: Adapter Bootstrap (3750-9999) ~11 epochs  
      - 10000 # Stage 2: Notes Bus Enable / SNC Training (10000-24999) - EXTENDED (15k steps)
      - 25000 # Stage 3: Rollback Training / Coverage+Agreement (25000-50000) - EXTENDED (25k steps)
      # Stage 4 removed: Trunk unfreezing requires >178GB VRAM (not feasible on B200s)
  stage_policies:
    0:
      name: "planner_pretrain"
      freeze:
        - "trunk"
        - "stream_adapters"
        - "cross_attention"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
        - "stream_classifier"
      unfreeze:
        - "planner_head"
        - "notes_head"
    1:
      name: "role_bootstrap"
      bus_mix_prob: 1.0  # Blind Spot Protection: Force Teacher Notes while Speculation is frozen
      freeze:
        - "trunk"
        - "speculation_head"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "stream_adapters"
        - "cross_attention"
        - "stream_classifier"
    2:
      name: "notes_bus_enable_extended"
      bus_mix_prob: 0.75  # Force teacher notes to jumpstart usage
      freeze:
        - "trunk"
        - "agreement_head"
        - "coverage_head"
      unfreeze:
        - "speculation_head"
    3:
      name: "rollback_training_extended"
      bus_mix_prob: 0.35
      stream_dropout_prob: 0.15
      freeze:
        - "trunk"  # Trunk remains frozen - parameter-efficient training
      unfreeze:
        - "agreement_head"
        - "coverage_head"
  loss_weights:
    kd: 2.0
    stab: 0.1
    use: 1.0
    cov: 1.0
    nli: 0.05
    red: 0.0
    spec_kl: 0.1
    stream: 0.0
    agree: 1.0
  coverage_threshold: 0.4
  bus_mix_prob: 0.0
  stream_dropout_prob: 0.0
  parallel_micro_steps: 0
  notes_noise:
    drop_p: 0.0
    paraphrase_p: 0.0
  negative_sampling:
    enabled: false
    start_stage: 3
    contradiction_ratio: 0.5
    max_contradictions: 4
    noise_ratio: 0.1
    noise_std: 0.05
  nli_scorer: null
  metrics:
    mask_ablation_every: 100
    stability_every: 501  # Avoids collision with stage transitions (3750, 10000, 17500)
  gradnorm:
    enabled: false
    target_ratio: 1.0
    alpha: 0.05
    min_scale: 0.1
    max_scale: 5.0
  nli_margin: 0.1
  spec_kl_temperature: 1.0
  kd_temperature_planner: 1.0
  kd_temperature_lm: 0.5
