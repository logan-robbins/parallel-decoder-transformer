@inproceedings{Ainslie2023,
  author = {Ainslie, J. and Lee-Thorp, J. and de Jong, M. and Zemlyanskiy, Y. and Lebrón, F. and Sanghai, S.},
  title = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  booktitle = {Proceedings of EMNLP 2023},
  year = {2023},
  note = {arXiv:2305.13245}
}

@article{Belinkov2022,
  author = {Belinkov, Y.},
  title = {Probing Classifiers: Promises, Shortcomings, and Advances},
  journal = {Computational Linguistics},
  volume = {48},
  number = {1},
  pages = {207--219},
  year = {2022},
  note = {arXiv:2102.12452}
}

@article{Beltagy2020,
  author = {Beltagy, I. and Peters, M. E. and Cohan, A.},
  title = {Longformer: The Long-Document Transformer},
  year = {2020},
  note = {arXiv:2004.05150}
}

@book{Cover2006,
  author = {Cover, T. M. and Thomas, J. A.},
  title = {Elements of Information Theory},
  edition = {2nd},
  publisher = {Wiley-Interscience},
  year = {2006}
}

@article{Dai2019,
  author = {Dai, Z. and Yang, Z. and Yang, Y. and Carbonell, J. and Le, Q. V. and Salakhutdinov, R.},
  title = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  year = {2019},
  note = {arXiv:1901.02860}
}

@article{Dao2023,
  author = {Dao, T. and Fu, D. Y. and Ermon, S. and Rudra, A. and Ré, C.},
  title = {FlashAttention-2: Faster Attention with Better Memory Utilization},
  year = {2023},
  note = {arXiv:2307.08691}
}

@article{Medusa2024,
  author = {Cai, T. and others},
  title = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},
  year = {2024},
  note = {arXiv:2401.10774}
}

@article{EAGLE2024,
  author = {Sun, Z. and others},
  title = {EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty},
  year = {2024},
  note = {arXiv:2401.15077}
}

@inproceedings{PSLM2024,
  author = {Liu, X. and others},
  title = {PSLM: Parallel Generation of Text and Speech with LLMs},
  booktitle = {Findings of EMNLP 2024},
  year = {2024}
}

@article{Goyal2021,
  author = {Goyal, A. and others},
  title = {Coordination Among Neural Modules Through a Shared Workspace},
  year = {2021},
  note = {arXiv:2103.01197}
}

@article{Vapnik2015,
  author = {Vapnik, V.},
  title = {Learning Using Privileged Information: Similarity Control and Knowledge Transfer},
  journal = {Journal of Machine Learning Research},
  volume = {16},
  year = {2015}
}

@inproceedings{LopezPaz2015,
  author = {Lopez-Paz, D. and Bottou, L. and Schölkopf, B. and Vapnik, V.},
  title = {Unifying Distillation and Privileged Information},
  booktitle = {ICLR Workshop},
  year = {2016}
}

@article{Deng2025,
  author = {Deng, J.},
  title = {Latent Reasoning in LLMs as a Vocabulary-Space Superposition},
  year = {2025},
  note = {arXiv:2510.15522}
}

@inproceedings{Geifman2017,
  author = {Geifman, Y. and El-Yaniv, R.},
  title = {Selective classification for deep neural networks},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS 2017)},
  year = {2017},
  note = {arXiv:1705.08500}
}

@article{Jin2025,
  author = {Jin, C. and Rinott, R. and Pasunuru, R.},
  title = {PASTA: Post-Alignment Steering to Adapt Implicit Assumptions in Textual Alignment},
  year = {2025},
  note = {arXiv:2505.12205}
}

@article{Kang2025,
  author = {Kang, H.},
  title = {LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning},
  year = {2025},
  note = {arXiv:2510.04573}
}

@inproceedings{Kwon2023,
  author = {Kwon, W. and Li, Z. and Zhuang, S. and Sheng, Y. and Zheng, L. and Yu, C. H. and Gonzalez, J. and Zhang, H. and Stoica, I.},
  title = {Efficient memory management for large language model serving with PagedAttention},
  booktitle = {Proceedings of SOSP 2023},
  year = {2023},
  note = {arXiv:2309.06180}
}

@article{Liu2025a,
  author = {Liu, X. and others},
  title = {ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference},
  year = {2025},
  note = {arXiv:2502.00299}
}

@article{Liu2025b,
  author = {Liu, X. and others},
  title = {ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs},
  year = {2025},
  note = {arXiv:2503.10714}
}

@article{Ning2023,
  author = {Ning, X. and Lin, Z. and Yang, H. and Wang, Y.},
  title = {Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation},
  year = {2023},
  note = {arXiv:2307.15337}
}

@article{Ren2023,
  author = {Ren, A. and Li, B. and Sun, R. and Liu, T.},
  title = {Calibrating Large Language Models with Handcrafted Augmentation},
  year = {2023},
  note = {arXiv:2310.05417}
}

@article{Stern2018,
  author = {Stern, M. and Shazeer, N. and Uszkoreit, J.},
  title = {Blockwise Parallel Decoding for Deep Autoregressive Models},
  year = {2018},
  note = {arXiv:1811.03115}
}

@article{Yan2024,
  author = {Yan, M. and others},
  title = {Decoding Speculative Decoding},
  year = {2024},
  note = {arXiv:2402.01528}
}

@article{Yoshikawa2023,
  author = {Yoshikawa, S. and Okazaki, N.},
  title = {Selective Generation for Pass-fail Evaluation},
  year = {2023},
  note = {arXiv:2311.08803}
}

@article{survey2025,
  author = {Zhang, L. and Fang, L. and Duan, C. and He, M. and Pan, L. and Xiao, P. and Huang, S. and Zhai, Y. and Hu, X. and Yu, P. S. and Liu, A.},
  title = {A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models},
  year = {2025},
  note = {arXiv:2508.08712}
}

@inproceedings{Li2024lookahead,
  author = {Li, Y. and Wei, F. and Zhang, C. and Zhang, H.},
  title = {Break the Sequential Dependency of LLM Inference Using Lookahead Decoding},
  booktitle = {Proceedings of ICML 2024},
  year = {2024},
  note = {arXiv:2402.02057}
}

@article{Rodionov2025,
  author = {Rodionov, A. and others},
  title = {Hogwild! Inference: Parallel LLM Generation via Concurrent Attention},
  year = {2025},
  note = {arXiv:2504.06261}
}

@article{Xiao2025sprint,
  author = {Xiao, G. and others},
  title = {SPRINT: Enabling Interleaved Planning and Parallelized Execution in Large Reasoning Models},
  year = {2025},
  note = {arXiv:2506.05745}
}

@article{Zheng2025,
  author = {Zheng, L. and others},
  title = {Semantic Reflective Verification for Faster Speculative Decoding},
  year = {2025},
  note = {arXiv:2505.18629}
}

@article{Wei2025sidechannel,
  author = {Wei, J. and others},
  title = {Side-Channel Attacks on Speculative Decoding in LLMs},
  year = {2025},
  note = {arXiv:2411.01076}
}

@inproceedings{Chen2018,
  author = {Chen, Z. and Badrinarayanan, V. and Lee, C.-Y. and Rabinovich, A.},
  title = {GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},
  booktitle = {Proceedings of ICML 2018},
  year = {2018},
  note = {arXiv:1711.02257}
}

@inproceedings{Miyato2018,
  author = {Miyato, T. and Kataoka, T. and Koyama, M. and Yoshida, Y.},
  title = {Spectral Normalization for Generative Adversarial Networks},
  booktitle = {Proceedings of ICLR 2018},
  year = {2018},
  note = {arXiv:1802.05957}
}

@inproceedings{Fazlyab2019,
  author = {Fazlyab, M. and Robey, A. and Hassani, H. and Morari, M. and Pappas, G. J.},
  title = {Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks},
  booktitle = {Proceedings of NeurIPS 2019},
  year = {2019},
  note = {arXiv:1906.04893}
}