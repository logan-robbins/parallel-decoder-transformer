\section{Speculative Note Conditioning}
\label{sec:snc}

To enable parallel coordination without modifying the pre-trained weights of the base language model (the ``trunk''), we introduce \textbf{Speculative Note Conditioning (SNC)}. SNC is a parameter-efficient architectural extension that allows parallel decoding streams to read and write to a shared, dynamic latent space called the \textit{Note State}. This approach is motivated by recent findings that LLMs maintain rich latent reasoning structures \cite{Deng2025}, which we explicitly surface and share across parallel streams.

Formally, let the frozen trunk be parameterized by $\theta_{\text{pre}}$. We freeze these parameters to preserve the general reasoning capabilities of the base model. We introduce a lightweight set of trainable parameters $\phi$, consisting of Stream Adapters, Note Heads, and the SNC Cross-Attention mechanism.

\subsection{Frozen Trunk and Stream Adapters}
Let $\mathbf{H}^{(k)}_l \in \mathbb{R}^{T \times d}$ denote the hidden states of stream $k$ at layer $l$. In a standard Transformer, $\mathbf{H}^{(k)}_{l+1} = \text{Block}_{\theta}(\mathbf{H}^{(k)}_l)$. In PDT, we inject stream-specific conditioning via \textbf{Stream Adapters} \cite{houlsby2019parameter}.

The adapter for stream $k$, denoted as $A_{\phi}^{(k)}$, is a bottleneck MLP with residual connection:
\begin{equation}
    \mathbf{H}^{(k)}_{l, \text{adapt}} = \mathbf{H}^{(k)}_l + \mathbf{W}_{\text{up}} \cdot \sigma(\mathbf{W}_{\text{down}} \cdot \text{LayerNorm}(\mathbf{H}^{(k)}_l))
\end{equation}
where $\sigma$ is the activation function (GELU). Crucially, these adapters allow the frozen trunk to process identical tokens differently depending on the stream index, breaking the symmetry of parallel decoding.

\subsection{SNC Cross-Attention Mechanism}
\label{sec:snc_attention}

The core coordination primitive is the Speculative Note Conditioning layer. Unlike standard decoding where context is strictly local (autoregressive), SNC allows stream $k$ to attend to the \textit{Speculative Note State} of sibling streams $\mathcal{J} = \{j \mid j \neq k\}$.

Let $\mathbf{N}^{(j)} \in \mathbb{R}^{M \times d_{\text{note}}}$ be the compressed note embeddings generated by stream $j$. We define the SNC mechanism as a residual cross-attention block injected between layers of the frozen trunk.

\paragraph{Query-Key-Value Construction.} 
The query $\mathbf{Q}^{(k)}$ is derived from the current stream's hidden state. The keys $\mathbf{K}^{(j)}$ and values $\mathbf{V}^{(j)}$ are projected from the sibling notes:
\begin{align}
    \mathbf{Q}^{(k)} &= \mathbf{H}^{(k)}_{l} \mathbf{W}_Q \\
    \mathbf{K}^{(j)} &= \mathbf{N}^{(j)} \mathbf{W}_K, \quad \mathbf{V}^{(j)} = \mathbf{N}^{(j)} \mathbf{W}_V
\end{align}
where $\mathbf{W}_{\{Q,K,V\}} \in \phi$ are learned projections.

\paragraph{Trust-Gated Residual Injection.} 
A critical challenge in adding attention to a frozen model is preserving the signal magnitude distribution. Unconstrained injection can destabilize the pre-trained features. We employ a \textbf{Zero-Initialization Gating} mechanism \cite{bachlechner2021rezero}.

The context vector $\mathbf{C}^{(k)}$ is computed via standard scaled dot-product attention over the union of sibling notes. The final update to the trunk's hidden state is:
\begin{equation}
    \tilde{\mathbf{H}}^{(k)}_{l} = \mathbf{H}^{(k)}_{l} + \lambda \cdot \mathbf{C}^{(k)} \mathbf{W}_O
\end{equation}
where $\lambda \in [0, 1]$ is a learnable scalar gate, initialized to $\sigma(\gamma)$ with $\gamma \ll 0$ (e.g., $\gamma = -4.0$). This initialization ensures that at the start of training, $\lambda \approx 0$, effectively reducing the SNC mechanism to an identity function. This allows the model to retain its pre-trained performance ($\tilde{\mathbf{H}} \approx \mathbf{H}$) while gradually learning to incorporate cross-stream information via backpropagation.

\subsection{Speculative Invariance via Agreement Heads}
\label{sec:agreement}

While the SNC attention mechanism allows information flow, it does not guarantee correctness. A sibling stream may write a ``hallucinated'' note that contradicts the global context. To manage this, we introduce the \textbf{Agreement Head}, a scalar classifier trained to estimate the consistency between a stream's generation and the ground truth (or teacher execution trace). This approach draws inspiration from selective classification \cite{Geifman2017} and model calibration techniques \cite{Ren2023}, where auxiliary heads learn to estimate prediction confidence.

For a hidden state $\mathbf{h}_t$, the Agreement Head predicts a trust score $s_t \in [0, 1]$:
\begin{equation}
    s_t = \sigma(\mathbf{w}_{\text{agree}}^T \cdot \text{Dropout}(\mathbf{h}_t) + b_{\text{agree}})
\end{equation}
During inference, this score serves as the \textit{Speculative Invariant}. If $s_t < \tau$ (where $\tau$ is a tuned threshold), the system identifies a coherence failure. Unlike ``soft'' attention gating, this triggers a discrete \textbf{Rollback} operation, pruning the divergent stream and forcing a regeneration or synchronization event. This verification mechanism complements recent work on semantic verification for speculative decoding \cite{Zheng2025}.

This separation of concerns---using SNC Attention for \textit{information flow} and Agreement Heads for \textit{control flow}---allows the PDT architecture to be both expressive and robust, recovering approximate serial semantics with bounded error (as proven in Appendix A).