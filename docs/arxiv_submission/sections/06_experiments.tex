\section{Experiments and Results}
\label{sec:experiments}

We evaluate PDT on a curriculum of 50,000 steps, designed to stress-test the coordination mechanisms under strict memory constraints. Our primary experimental goal is to determine if a frozen 20B-parameter trunk can learn to self-correct parallel inconsistencies via lightweight adapters.

\subsection{Experimental Setup}
\textbf{Model \& Hardware.} We use the 20B-parameter \texttt{GPT-OSS} model as our frozen backbone. Training was conducted on a cluster of 8$\times$NVIDIA B200 GPUs (180GB VRAM each). We use a global batch size of 16 (micro-batch 1) with gradient checkpointing enabled.\footnote{Full training logs and metrics: \url{https://wandb.ai/ljrweb-self/parallel-decoder-transformer/runs/fmuea63a}}
\textbf{Dataset.} We constructed a dataset of 10,000 multi-section reasoning tasks distilled from GPT-4. Each example includes a ``Teacher Plan'' and a ``Notes Contract''---a structured set of semantic commitments that the parallel streams are expected to fulfill.

\subsection{The Memory Cliff: Validating Parameter Efficiency}
\label{sec:exp_memory}
A core motivation for SNC is the prohibitive cost of full-model fine-tuning. We empirically mapped the memory requirements of the B200 architecture across our curriculum stages.
While adapter-based training (Stages 0--3) remained stable at $\approx$171GB utilization, the transition to full fine-tuning (Stage 4) precipitated an immediate Out-Of-Memory (OOM) failure, requiring an estimated $>$290GB per device to store optimizer states for the 20B trunk.
This result confirms that for models of this scale, parameter-efficient coordination is not merely an optimization but a hard requirement for single-node deployability.

\subsection{SNC Convergence and Self-Correction}
\label{sec:exp_convergence}
We successfully trained the SNC mechanism for 30,000 steps (Stages 2 and 3). The critical metric for coordination is the \texttt{coverage\_loss}, which measures the \texttt{coverage\_head}'s ability to detect when a generated section fulfills a specific item in the Note State.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/coverage_loss.png}
    \caption{Learning curve of the Coverage Mechanism (Stages 2--3). The precipitous drop in loss at Step 20k marks the emergence of successful cross-stream coordination. Precision converges to 77.8\% in the final stage.}
    \label{fig:coverage_loss}
\end{figure}

As shown in Figure \ref{fig:coverage_loss}, the model exhibits a classic phase transition:
\begin{itemize}
    \item \textbf{Initial Phase (Steps 10k--20k):} The loss is high ($\approx$656.0) as the randomized heads fail to align with the frozen trunk's features.
    \item \textbf{Discovery Phase (Steps 20k--25k):} We observe a rapid descent in loss as the \texttt{Stream Adapters} learn to project the trunk's hidden states into the Note Bus subspace.
    \item \textbf{Convergence (Steps 40k+):} The loss plateaus at $\approx$0.2, indicating that the parallel streams have successfully learned to ``agree'' on semantic completion.
\end{itemize}

\subsection{Precision-Recall Analysis}
At the final checkpoint (Step 50,000), we evaluated the \texttt{coverage\_head} on a held-out validation set.
\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Coverage Precision & \textbf{77.78\%} \\
Coverage Recall & 4.91\% \\
Validation Loss & 0.00 \\
\bottomrule
\end{tabular}
\caption{Performance of the SNC Coverage Mechanism at Step 50k.}
\label{tab:results}
\end{table}

The high precision (\textbf{77.8\%}) is the most significant result. It indicates that when the system flags a plan item as ``Covered,'' it is highly likely to be correct. The low recall suggests the model is conservative---preferring to under-claim coverage rather than hallucinate progress. In a parallel rollback system, this conservatism is a desirable safety property, preventing the "Coherence Drift" observed in baselines like Skeleton-of-Thought.